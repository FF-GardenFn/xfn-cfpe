# ARENA

Multi-turn evaluation framework for cross-provider model comparison on extended reasoning tasks. Tests Claude against other provider models in debates, collaborative problem-solving, and long-threaded conversations. Uses the evaluation equation from `equ.md` to measure total operational cost including turns, clarifications, and error rates.

**Status**: Planned

# TODO: 
[] Extend multi_turn_evaluator.py for cross-provider matchups
[] Implement debate protocol with alternating model responses
[] Add scoring based on evaluation equation metrics
